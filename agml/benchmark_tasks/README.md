# AgML Benchmark Tasks

We would like to support many AgML benchmark tasks. For each task, there will be
* datasets
* models
* evaluation metrics

Should we keep these per task or make them generic?

