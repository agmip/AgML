# Repository structure

We should be able to support multiple benchmark tasks. For each tasks, we need to support
* datasets (including data cards and data splits)
* models
* evaluation metrics

Should we make these per task or keep them generic?
